# -*- coding: utf-8 -*-
"""LSTM-KALMAN FILTER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Te5C8WunD2LPVOplIfIpFNrsN8PPA65O
"""

pip install pykalman

pip install hydroeval

#load data trian
from google.colab import files
uploaded = files.upload()

#importing pandas library
import pandas as pd#read data from a file with path ('filename.csv')
# input trian
trianx1= pd.read_csv('K-TRIANQT-5.csv')#to view the first 10 rows of the dataset
trianx2= pd.read_csv('K-TRIANQT-4.csv')#to view the first 10 rows of the dataset
trianx3= pd.read_csv('K-TRIANQT-3.csv')#to view the first 10 rows of the dataset
trianx4= pd.read_csv('K-TRIANQT-2.csv')#to view the first 10 rows of the dataset
trianx5= pd.read_csv('K-TRIANQT-1.csv')#to view the first 10 rows of the dataset
# output trian
triany= pd.read_csv('K-TRIANQT.csv')#to view the first 10 rows of the dataset
print(trianx1)
print(trianx2)
print(trianx3)
print(trianx4)
print(trianx5)
print(triany)

# Commented out IPython magic to ensure Python compatibility.
import numpy
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(15,10))
# %matplotlib inline
import mpld3
mpld3.enable_notebook()

line1, = plt.plot(DATA, label="Observations", linestyle='-')
plt.ylabel('debi')
plt.xlabel('days')

plt.legend(handles=[line1], bbox_to_anchor=(0.1,1.02, 1.5, .10), loc=3, borderaxespad=0., mode='expand',  ncol=3)
plt.show()

### normalize 
# normalize dataset with MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
trianx1 = scaler.fit_transform(trianx1)
trianx2 = scaler.fit_transform(trianx2)
trianx3 = scaler.fit_transform(trianx3)
trianx4 = scaler.fit_transform(trianx4)
trianx5 = scaler.fit_transform(trianx5)
trianynorm = scaler.fit_transform(triany)

#print(trianynorm)

##### dataset  normaliaze
from numpy import array
from numpy import hstack
in_seq1 = trianx1.reshape((len(trianx1), 1))
in_seq2 = trianx2.reshape((len(trianx2), 1))
in_seq3 = trianx3.reshape((len(trianx3), 1))
in_seq4 = trianx4.reshape((len(trianx4), 1))
in_seq5 = trianx5.reshape((len(trianx5), 1))


out_seq = trianynorm.reshape((len(trianynorm), 1))

datasettrian = hstack((in_seq1, in_seq2, in_seq3 ,in_seq4 ,in_seq5 ,out_seq))
#print(datasettrian)

# multivariate lstm example
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense

# split a multivariate sequence into samples
def split_sequences(sequences, n_steps):
	X, y = list(), list()
	for i in range(len(sequences)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the dataset
		if end_ix > len(sequences):
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
n_steps =1
# convert into input/output
X , y = split_sequences(datasettrian,n_steps)  #### scaled  or datasettrian
#print(X)
#print(y)

# define model lstm  for trian
#the dataset knows the number of features, e.g. 2

n_features = X.shape[2]
# define model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=200, verbose=0)

### predict lstm -trian
predicttrain_lstm = model.predict(X, verbose=0)
#print(predicttrain_lstm )

### denormalize predicttrain_lstm
import numpy
import matplotlib.pyplot as plt
import pandas 
import math
from keras.models import Sequential
from keras.layers import Dense 
from keras.layers import LSTM 
from sklearn.preprocessing import MinMaxScaler 

#predicttrain_lstm1= predicttrain_lstm.reshape((len(predicttrain_lstm), 1))

trainPredict = scaler.inverse_transform(predicttrain_lstm)
#print(trainPredict)

import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from hydroeval import evaluator, nse, mare

rmse_lstm = math.sqrt(mean_squared_error(triany,trainPredict))
trainScore1_r2 = r2_score(triany, trainPredict)
print('Score lstm: %.7f RMSE and %.7f R-Square' % (rmse_lstm, trainScore1_r2))
### NASH
nash_lstm = evaluator(nse, triany,trainPredict)
print('nash_lstm: %.2f Nash-Sutcliffe' % nash_lstm)

#################  MAE
mae_lstm = mean_absolute_error( triany,trainPredict)
print('mae  lstm: %.2f MAE' % mae_lstm)
########### AARE
aare_lstm = evaluator(mare,  triany,trainPredict)
print('aare lstm: %.2f mean absolute relative error' % aare_lstm)

###### DEFINE KF MODEL  
import numpy as np
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from pykalman import KalmanFilter
import pandas
# https://pykalman.github.io/class_docs.html

#df = pd.DataFrame(trainPredict)
#covariances = df.cov()
#covariances  = numpy.cov(trainPredict)
mean_1 = numpy.mean(trainPredict)
#print(covariances )
#print(initial_state_mean )
kf = KalmanFilter(initial_state_mean = mean_1 ,n_dim_obs=1 ) #    initial_state_mean=12.6   y[0]  https://blog.quantinsti.com/kalman-filter/
#kf = KalmanFilter(transition_matrices = [1],
#          observation_matrices = [1],
#           initial_state_mean = mean_1,
#          initial_state_covariance = 1,
#       observation_covariance=1,
#             transition_covariance=.0001)
#kf = KalmanFilter.filter_update() 
#kf = kf.em(trainPredict, n_iter=50, em_vars='all')
predictedtrian_LSTM_kalman_smooth = (kf.smooth(trainPredict)[0])#[:, 0]
#print(predictedtrian_LSTM_kalman)
predictedtrian_LSTM_kalman = (kf.filter(trainPredict)[0])#[:, 0]

##########

########## RMSE TRIAN-LSTM-KF 
import numpy as np
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from hydroeval import evaluator, nse, mare


### rmse  r2
rmse_lstm_kfsmooth = math.sqrt(mean_squared_error( triany,predictedtrian_LSTM_kalman_smooth))
trainScore2_r2smooth = r2_score(triany,predictedtrian_LSTM_kalman_smooth)
print('Score lstm-kfsmooth: %.2f RMSE and %.2f R-Square' % (rmse_lstm_kfsmooth, trainScore2_r2smooth))
###
rmse_lstm_kf = math.sqrt(mean_squared_error( triany,predictedtrian_LSTM_kalman ))
trainScore2_r2 = r2_score(triany,predictedtrian_LSTM_kalman)
print('Score lstm-kf: %.2f RMSE and %.2f R-Square' % (rmse_lstm_kf, trainScore2_r2))

###################nash
nash_lstm_kfsmooth = evaluator(nse,triany,predictedtrian_LSTM_kalman_smooth)
print('nash_lstm-kfsmooth: %.2f Nash-Sutcliffe' % nash_lstm_kfsmooth)


nash_lstm_kf = evaluator(nse,triany,predictedtrian_LSTM_kalman)
print('nash_lstm-kf: %.2f Nash-Sutcliffe' % nash_lstm_kf)
#################  mae 
mae_lstm_kfsmooth = mean_absolute_error(triany,predictedtrian_LSTM_kalman_smooth)
print('mae  lstm-kfsmooth: %.2f MAE' % mae_lstm_kfsmooth)

mae_lstm_kf = mean_absolute_error(triany,predictedtrian_LSTM_kalman)
print('mae  lstm-kf: %.2f MAE' % mae_lstm_kf)

########### aare 
aare_lstm_kfsmooth = evaluator(mare, triany,predictedtrian_LSTM_kalman_smooth)
print('aare lstm_kfsmooth: %.2f mean absolute relative error' % aare_lstm_kfsmooth)

aare_lstm_kf = evaluator(mare, triany,predictedtrian_LSTM_kalman)
print('aare lstm_kf: %.2f mean absolute relative error' % aare_lstm_kf)

#### define ukf model 
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from pykalman import KalmanFilter
from pykalman import UnscentedKalmanFilter
from pykalman import AdditiveUnscentedKalmanFilter

ukf = UnscentedKalmanFilter(initial_state_mean = mean_1)

measurements_predicted_ukfsmooth = (ukf.smooth(trainPredict)[0])#[:, 0]
#print(measurements_predicted_ukf)
measurements_predicted_ukf = (ukf.filter(trainPredict)[0])#[:, 0]

#aukf = AdditiveUnscentedKalmanFilter(initial_state_mean = mean_1)
#measurements_predicted_aukfsmooth = (aukf.smooth(trainPredict)[0])#[:, 0]
#print(measurements_predicted_aukf)
#measurements_predicted_aukf = (aukf.filter(trainPredict)[0])#[:, 0]

######### RMSE TRIAN-LSTM-uKF 
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score

rmse_lstm_ukfsmooth = math.sqrt(mean_squared_error( triany,measurements_predicted_ukfsmooth))
trainScore3_r2smooth = r2_score(triany, measurements_predicted_ukfsmooth)
print('Score lstm-ukfsmooth: %.2f RMSE and %.2f R-Square' % (rmse_lstm_ukfsmooth, trainScore3_r2smooth))


rmse_lstm_ukf = math.sqrt(mean_squared_error( triany,measurements_predicted_ukf))
trainScore3_ur2 = r2_score(triany, measurements_predicted_ukf)
print('Score lstm-ukf: %.2f RMSE and %.2f R-Square' % (rmse_lstm_ukf, trainScore3_ur2))

##################   nash 

nash_lstm_ukf = evaluator(nse,triany, measurements_predicted_ukf)
print('nash_lstm-ukf: %.2f Nash-Sutcliffe' % nash_lstm_ukf)

nash_lstm_ukfsmooth = evaluator(nse,triany, measurements_predicted_ukfsmooth)
print('nash_lstm-ukfsmooth: %.2f Nash-Sutcliffe' % nash_lstm_ukfsmooth)

#################  mae 
mae_lstm_ukfsmooth = mean_absolute_error(triany, measurements_predicted_ukfsmooth)
print('mae  lstm-ukfsmooth: %.2f MAE' % mae_lstm_ukfsmooth)

mae_lstm_ukf = mean_absolute_error(triany, measurements_predicted_ukf)
print('mae  lstm-ukf: %.2f MAE' % mae_lstm_ukf)
 

########### aare 
aare_lstm_ukfsmooth = evaluator(mare, triany , measurements_predicted_ukfsmooth)
print('aare lstm_ukfsmooth: %.2f mean absolute relative error' % aare_lstm_ukfsmooth)

aare_lstm_ukf = evaluator(mare, triany , measurements_predicted_ukf)
print('aare lstm_ukf: %.2f mean absolute relative error' % aare_lstm_ukf)

#SCATER PLOT 
import numpy as np
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(15,10))

#plt.scatter(triany,predictedtrian_LSTM_kalman_smooth)
plt.title('R-Square=0.91')
#plt.scatter(triany,predictedtrian_LSTM_kalman)
#plt.scatter(triany,trainPredict)
#plt.scatter(triany,measurements_predicted_ukf)
plt.scatter(triany,measurements_predicted_ukfsmooth)

plt.ylabel('prediction')
plt.xlabel('observation')
plt.show()

pip install mpld3

# Commented out IPython magic to ensure Python compatibility.
import numpy
import matplotlib.pyplot as plt
#plt.figure(figsize=(15,10))
# %matplotlib inline
import mpld3
mpld3.enable_notebook()
fig, ax = plt.subplots(figsize=(15,10))


ax = plt.subplot(212)
line1, = plt.plot(triany, label="Observations", linestyle='--')

line2, = plt.plot(predictedtrian_LSTM_kalman_smooth, label=" LSTM-KFsmooth", linestyle='-.')
line3, = plt.plot(trainPredict, label="LSTM", linestyle=':')

line4, = plt.plot(predictedtrian_LSTM_kalman, label="LSTM-KF", linestyle=':')

line5, = plt.plot(measurements_predicted_ukf, label="LSTM-UKF", linestyle=':')
line6, = plt.plot(measurements_predicted_ukfsmooth, label="LSTM-UKFsmooth", linestyle=':')

#plt.title('debikvnjvnfjvn')
plt.legend(handles=[line1, line2, line3, line4, line5, line6], bbox_to_anchor=(0.01, 1.02, 0.5, .10), loc=6, borderaxespad=0., mode='expand',
           ncol=3)


plt.ylabel('debi')
plt.xlabel('time(days)')
plt.show()

import numpy as np

stdtriany=numpy.std(triany)
print(stdtriany)

stdtrainPredict=numpy.std(trainPredict)
print(stdtrainPredict)

stdkfS=numpy.std(predictedtrian_LSTM_kalman_smooth)
print(stdkfS)


stdukfS=numpy.std(measurements_predicted_ukfsmooth)
print(stdukfS)
stdukf=numpy.std(measurements_predicted_ukf)
print(stdukf)
stdkf=numpy.std(predictedtrian_LSTM_kalman)
print(stdkf)

#!/usr/bin/env python
# Copyright: This document has been placed in the public domain.

"""
Taylor diagram (Taylor, 2001) implementation.

Note: If you have found these software useful for your research, I would
appreciate an acknowledgment.
"""

__version__ = "Time-stamp: <2018-12-06 11:43:41 ycopin>"
__author__ = "Yannick Copin <yannick.copin@laposte.net>"

import numpy as NP
import matplotlib.pyplot as PLT


class TaylorDiagram(object):
    """
    Taylor diagram.

    Plot model standard deviation and correlation to reference (data)
    sample in a single-quadrant polar plot, with r=stddev and
    theta=arccos(correlation).
    """

    def __init__(self, refstd,
                 fig=None, rect=111, label='_', srange=(0, 1.5), extend=False):
        """
        Set up Taylor diagram axes, i.e. single quadrant polar
        plot, using `mpl_toolkits.axisartist.floating_axes`.

        Parameters:

        * refstd: reference standard deviation to be compared to
        * fig: input Figure or None
        * rect: subplot definition
        * label: reference label
        * srange: stddev axis extension, in units of *refstd*
        * extend: extend diagram to negative correlations
        """

        from matplotlib.projections import PolarAxes
        import mpl_toolkits.axisartist.floating_axes as FA
        import mpl_toolkits.axisartist.grid_finder as GF

        self.refstd = refstd            # Reference standard deviation

        tr = PolarAxes.PolarTransform()

        # Correlation labels
        rlocs = NP.array([0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1])
        if extend:
            # Diagram extended to negative correlations
            self.tmax = NP.pi
            rlocs = NP.concatenate((-rlocs[:0:-1], rlocs))
        else:
            # Diagram limited to positive correlations
            self.tmax = NP.pi/2
        tlocs = NP.arccos(rlocs)        # Conversion to polar angles
        gl1 = GF.FixedLocator(tlocs)    # Positions
        tf1 = GF.DictFormatter(dict(zip(tlocs, map(str, rlocs))))

        # Standard deviation axis extent (in units of reference stddev)
        self.smin = srange[0] * self.refstd
        self.smax = srange[1] * self.refstd

        ghelper = FA.GridHelperCurveLinear(
            tr,
            extremes=(0, self.tmax, self.smin, self.smax),
            grid_locator1=gl1, tick_formatter1=tf1)

        if fig is None:
            fig = PLT.figure()

        ax = FA.FloatingSubplot(fig, rect, grid_helper=ghelper)
        fig.add_subplot(ax)

        # Adjust axes
        ax.axis["top"].set_axis_direction("bottom")   # "Angle axis"
        ax.axis["top"].toggle(ticklabels=True, label=True)
        ax.axis["top"].major_ticklabels.set_axis_direction("top")
        ax.axis["top"].label.set_axis_direction("top")
        ax.axis["top"].label.set_text("Correlation")

        ax.axis["left"].set_axis_direction("bottom")  # "X axis"
        ax.axis["left"].label.set_text("Standard deviation")

        ax.axis["right"].set_axis_direction("top")    # "Y-axis"
        ax.axis["right"].toggle(ticklabels=True)
        ax.axis["right"].major_ticklabels.set_axis_direction(
            "bottom" if extend else "left")

        if self.smin:
            ax.axis["bottom"].toggle(ticklabels=False, label=False)
        else:
            ax.axis["bottom"].set_visible(False)          # Unused

        self._ax = ax                   # Graphical axes
        self.ax = ax.get_aux_axes(tr)   # Polar coordinates

        # Add reference point and stddev contour
        l, = self.ax.plot([0], self.refstd, 'k*',
                          ls='', ms=10, label=label)
        t = NP.linspace(0, self.tmax)
        r = NP.zeros_like(t) + self.refstd
        self.ax.plot(t, r, 'k--', label='_')

        # Collect sample points for latter use (e.g. legend)
        self.samplePoints = [l]

    def add_sample(self, stddev, corrcoef, *args, **kwargs):
        """
        Add sample (*stddev*, *corrcoeff*) to the Taylor
        diagram. *args* and *kwargs* are directly propagated to the
        `Figure.plot` command.
        """

        l, = self.ax.plot(NP.arccos(corrcoef), stddev,
                          *args, **kwargs)  # (theta, radius)
        self.samplePoints.append(l)

        return l

    def add_grid(self, *args, **kwargs):
        """Add a grid."""

        self._ax.grid(*args, **kwargs)

    def add_contours(self, levels=5, **kwargs):
        """
        Add constant centered RMS difference contours, defined by *levels*.
        """

        rs, ts = NP.meshgrid(NP.linspace(self.smin, self.smax),
                             NP.linspace(0, self.tmax))
        # Compute centered RMS difference
        rms = NP.sqrt(self.refstd**2 + rs**2 - 2*self.refstd*rs*NP.cos(ts))

        contours = self.ax.contour(ts, rs, rms, levels, **kwargs)

        return contours

def test1():
    """Display a Taylor diagram in a separate axis."""

    # Reference dataset
    x = NP.linspace(0, 4*NP.pi, 100)
    data = NP.sin(x)
    refstd = data.std(ddof=1)           # Reference standard deviation

    # Generate models
    m1 = data + 0.2*NP.random.randn(len(x))     # Model 1
    m2 = 0.8*data + .1*NP.random.randn(len(x))  # Model 2
    m3 = NP.sin(x-NP.pi/10)                     # Model 3

    # Compute stddev and correlation coefficient of models
    samples = NP.array([ [m.std(ddof=1), NP.corrcoef(data, m)[0, 1]]
                         for m in (m1, m2, m3)])

    fig = PLT.figure(figsize=(10, 4))

    ax1 = fig.add_subplot(1, 2, 1, xlabel='X', ylabel='Y')
    # Taylor diagram
    dia = TaylorDiagram(refstd, fig=fig, rect=122, label="Reference",
                        srange=(0.5, 1.5))

    colors = PLT.matplotlib.cm.jet(NP.linspace(0, 1, len(samples)))

    ax1.plot(x, data, 'ko', label='Data')
    for i, m in enumerate([m1, m2, m3]):
        ax1.plot(x, m, c=colors[i], label='Model %d' % (i+1))
    ax1.legend(numpoints=1, prop=dict(size='small'), loc='best')

    # Add the models to Taylor diagram
    for i, (stddev, corrcoef) in enumerate(samples):
        dia.add_sample(stddev, corrcoef,
                       marker='$%d$' % (i+1), ms=10, ls='',
                       mfc=colors[i], mec=colors[i],
                       label="Model %d" % (i+1))

    # Add grid
    dia.add_grid()

    # Add RMS contours, and label them
    contours = dia.add_contours(colors='0.5')
    PLT.clabel(contours, inline=1, fontsize=10, fmt='%.2f')

    # Add a figure legend
    fig.legend(dia.samplePoints,
               [ p.get_label() for p in dia.samplePoints ],
               numpoints=1, prop=dict(size='small'), loc='upper right')

    return dia



def test2():
    """
    Climatology-oriented example (after iteration w/ Michael A. Rawlins).
    """

    # Reference std
    stdref = 226.39

    # Samples std,rho,name
    samples = [[211.14, 0.87, "LSTM"],
               [201.40, 0.87, "LSTM-KF"],
               [201.64,0.90, "LSTM-KFS"],
               [206.72, 0.86, "LSTM-UKF"],
               [204.50, 0.91, "LSTM-UKFS"]]
               
    fig = PLT.figure(figsize=(15, 10))

   # fig = PLT.figure()

    dia = TaylorDiagram(stdref, fig=fig, label='Reference',  rect=111) #extend=True)
    dia.samplePoints[0].set_color('r')  # Mark reference point as a red star

    # Add models to Taylor diagram
    for i, (stddev, corrcoef, name) in enumerate(samples):
        dia.add_sample(stddev, corrcoef,
                       marker='$%d$' % (i+1), ms=10, ls='',
                       mfc='k', mec='k',
                       label=name)

    # Add RMS contours, and label them
    contours = dia.add_contours(levels=5, colors='0.5')  # 5 levels in grey
    PLT.clabel(contours, inline=3, fontsize=7, fmt='%.0f')

    dia.add_grid()                                  # Add grid
    dia._ax.axis[:].major_ticks.set_tick_out(True)  # Put ticks outward

    # Add a figure legend and title
    fig.legend(dia.samplePoints,
               [ p.get_label() for p in dia.samplePoints ],
               numpoints=1, prop=dict(size='16'), loc='upper right')
    fig.suptitle("Taylor diagram", size='x-large')  # Figure title

    return dia


if __name__ == '__main__':

  #  dia = test1()
    dia = test2()

    PLT.show()

#####################################################################################################################################################

################ load data test 
#load data trian
from google.colab import files
uploaded = files.upload()

#importing pandas library
import pandas as pd#read data from a file with path ('filename.csv')
# input trian
testx1= pd.read_csv('K-TESTQT-5.csv')#to view the first 10 rows of the dataset
testx2= pd.read_csv('K-TESTQT-4.csv')#to view the first 10 rows of the dataset
testx3= pd.read_csv('K-TESTQT-3.csv')#to view the first 10 rows of the dataset
testx4= pd.read_csv('K-TESTQT-2.csv')#to view the first 10 rows of the dataset
testx5= pd.read_csv('K-TESTQT-1.csv')#to view the first 10 rows of the dataset
# output trian
testy= pd.read_csv('K-TESTQT.csv')#to view the first 10 rows of the dataset
#print(testx2)
#print(testy)

### normalize 
# normalize dataset with MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
testx1 = scaler.fit_transform(testx1)
testx2 = scaler.fit_transform(testx2)
testx3 = scaler.fit_transform(testx3)
testx4 = scaler.fit_transform(testx4)
testx5 = scaler.fit_transform(testx5)
testynorm = scaler.fit_transform(testy)



#datasettrian_norm = scaler.fit_transform(datasettrian)
#print(testy)
#print(testynorm)

##### dataset bedon normaliaze
from numpy import array
from numpy import hstack
in_seq1 = testx1.reshape((len(testx1), 1))
in_seq2 = testx2.reshape((len(testx2), 1))
in_seq3 = testx3.reshape((len(testx3), 1))
in_seq4 = testx4.reshape((len(testx4), 1))
in_seq5 = testx5.reshape((len(testx5), 1))


out_seq = testynorm.reshape((len(testynorm), 1))#.values

datasettest = hstack((in_seq1, in_seq2, in_seq3 ,in_seq4 ,in_seq5 ,out_seq))
#print(datasettest)

# multivariate lstm example
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense

# split a multivariate sequence into samples
def split_sequences(sequences, n_steps):
	X, y = list(), list()
	for i in range(len(sequences)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the dataset
		if end_ix > len(sequences):
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
n_steps =1
# convert into input/output
testX , ytest = split_sequences(datasettest,n_steps)  #### scaled  or datasettrian
print(testX)
#print(ytest)

### predict lstm -trian
predicttest_lstm = model.predict(testX, verbose=0)
#print(predicttest_lstm )

### denormalize predicttrain_lstm
import numpy
import matplotlib.pyplot as plt
import pandas 
import math
from keras.models import Sequential
from keras.layers import Dense 
from keras.layers import LSTM 
from sklearn.preprocessing import MinMaxScaler 

#predicttrain_lstm1= predicttrain_lstm.reshape((len(predicttrain_lstm), 1))

testPredict = scaler.inverse_transform(predicttest_lstm)
#print(testPredict)

######### RMSE TRIAN-LSTM
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from hydroeval import evaluator, nse, mare
from sklearn.metrics import mean_absolute_error
from hydroeval import evaluator, nse, mare

rmsetest_lstm = math.sqrt(mean_squared_error(testy, testPredict))
testScore1_r2 = r2_score(testy, testPredict)

print('Score lstm: %.2f RMSE and %.2f R-Square' % (rmsetest_lstm, testScore1_r2))
########### nash trian lstm-lstm-kf

nashtest_lstm = evaluator(nse, testy, testPredict)
print('nash_lstm: %.2f Nash-Sutcliffe' % nashtest_lstm)

#################  mae 
maetest_lstm = mean_absolute_error(testy, testPredict )
print('mae  lstm: %.2f MAE' % maetest_lstm)
########### aare 
aaretest_lstm = evaluator(mare, testy, testPredict)
print('aare lstm: %.2f mean absolute relative error' % aaretest_lstm)

###### DEFINE KF MODEL  
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from pykalman import KalmanFilter
mean_2 = numpy.mean(testPredict)
print(mean_2)
kf = KalmanFilter(initial_state_mean = mean_2)
#print(kf)
kf = kf.em(testPredict, n_iter=50, em_vars='all')

predictedtest_LSTM_kalmansmooth = (kf.smooth(testPredict)[0])#[:, 0]
predictedtest_LSTM_kalman = (kf.filter(testPredict)[0])#[:, 0]

#print(predictedtest_LSTM_kalman)

######## RMSE TRIAN-LSTM-KF 
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from hydroeval import evaluator, nse, mare

rmsetest_lstm_kf = math.sqrt(mean_squared_error(testy,predictedtest_LSTM_kalman))
testScore2_r2 = r2_score(testy,predictedtest_LSTM_kalman)
print('Score lstm-kf: %.2f RMSE and %.2f R-Square' % (rmsetest_lstm_kf, testScore2_r2))
#
rmsetest_lstm_kfsmooth = math.sqrt(mean_squared_error(testy,predictedtest_LSTM_kalmansmooth))
testScore2_r2smooth = r2_score(testy,predictedtest_LSTM_kalmansmooth)
print('Score lstm-kfsmooth: %.2f RMSE and %.2f R-Square' % (rmsetest_lstm_kfsmooth, testScore2_r2smooth))

##NASH
nashtest_lstm_kf = evaluator(nse, testy,predictedtest_LSTM_kalman)
print('nash_lstm-kf: %.2f Nash-Sutcliffe' % nashtest_lstm_kf)
##

nashtest_lstm_kfem = evaluator(nse, testy,predictedtest_LSTM_kalmansmooth)
print('nash_lstm-kfem: %.2f Nash-Sutcliffe' % nashtest_lstm_kfem)
#################  mae 
maetest_lstm_kf = mean_absolute_error(testy, predictedtest_LSTM_kalman)
print('mae  lstm-kf: %.2f MAE' % maetest_lstm_kf)

maetest_lstm_kfem = mean_absolute_error(testy, predictedtest_LSTM_kalmansmooth)
print('mae  lstm-kfem: %.2f MAE' % maetest_lstm_kfem)
########### aare 
aaretest_lstm_kf = evaluator(mare, testy, predictedtest_LSTM_kalman)
print('aare lstm_kf: %.2f mean absolute relative error' % aaretest_lstm_kf)

aaretest_lstm_kfem = evaluator(mare, testy, predictedtest_LSTM_kalmansmooth)
print('aare lstm_kfem: %.2f mean absolute relative error' % aaretest_lstm_kfem)

#### define ukf model 
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score
from pykalman import KalmanFilter
from pykalman import UnscentedKalmanFilter

ukf = UnscentedKalmanFilter(initial_state_mean = mean_2)
measurementstest_predicted_ukfs = (ukf.smooth(testPredict)[0])#[:, 0]
measurementstest_predicted_ukf = (ukf.filter(testPredict)[0])#[:, 0]

#print(measurementstest_predicted_ukf)

######### RMSE TRIAN-LSTM-uKF 
import numpy
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error, r2_score

rmsetest_lstm_ukf = math.sqrt(mean_squared_error( testy,measurementstest_predicted_ukf))
testScore3_r2 = r2_score(testy, measurementstest_predicted_ukf)
print('Score lstm-ukf: %.2f RMSE and %.2f R-Square' % (rmsetest_lstm_ukf, testScore3_r2))

rmsetest_lstm_ukfS = math.sqrt(mean_squared_error( testy,measurementstest_predicted_ukfs))
testScore3_UKFSr2 = r2_score(testy, measurementstest_predicted_ukfs)
print('Score lstm-ukfS: %.2f RMSE and %.2f R-Square' % (rmsetest_lstm_ukfS, testScore3_UKFSr2))

##################nash 

nashtest_lstm_ukf = evaluator(nse,testy, measurementstest_predicted_ukf )
print('nash_lstm-ukf: %.2f Nash-Sutcliffe' % nashtest_lstm_ukf)

nashtest_lstm_ukfS = evaluator(nse,testy, measurementstest_predicted_ukfs )
print('nash_lstm-ukfS: %.2f Nash-Sutcliffe' % nashtest_lstm_ukfS)

#################  mae 
maetest_lstm_ukf = mean_absolute_error(testy, measurementstest_predicted_ukf)
print('mae  lstm-ukf: %.2f MAE' % maetest_lstm_ukf)

maetest_lstm_ukfS = mean_absolute_error(testy, measurementstest_predicted_ukfs)
print('mae  lstm-ukfS: %.2f MAE' % maetest_lstm_ukfS)
########### aare 
aaretest_lstm_ukf = evaluator(mare, testy, measurementstest_predicted_ukf)
print('aare lstm_ukf: %.2f mean absolute relative error' % aaretest_lstm_ukf)

aaretest_lstm_ukfS = evaluator(mare, testy, measurementstest_predicted_ukfs)
print('aare lstm_ukfS: %.2f mean absolute relative error' % aaretest_lstm_ukfS)

import numpy as np

stdtesty=numpy.std(testy)
print(stdtesty)

stdtestPredict=numpy.std(testPredict)
print(stdtestPredict)

stdkf=numpy.std(predictedtest_LSTM_kalman)
print(stdkf)
stdkf=numpy.std(predictedtest_LSTM_kalmansmooth)
print(stdkf)

stdukf=numpy.std( measurementstest_predicted_ukf)
print(stdukf)
stdukfS=numpy.std( measurementstest_predicted_ukfs)
print(stdukfS)

#SCATER PLOT 
import numpy as np
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(15,10))

plt.scatter(testy,testPredict)
plt.title('R-Square=0.79')
#plt.scatter(testy,predictedtest_LSTM_kalman)
#plt.scatter(testy,predictedtest_LSTM_kalmansmooth)
#plt.scatter(testy,measurementstest_predicted_ukf)
plt.scatter(testy,measurementstest_predicted_ukfs)

plt.ylabel('prediction')
plt.xlabel('observation')
plt.show()

#!/usr/bin/env python
# Copyright: This document has been placed in the public domain.

"""
Taylor diagram (Taylor, 2001) implementation.

Note: If you have found these software useful for your research, I would
appreciate an acknowledgment.
"""

__version__ = "Time-stamp: <2018-12-06 11:43:41 ycopin>"
__author__ = "Yannick Copin <yannick.copin@laposte.net>"

import numpy as NP
import matplotlib.pyplot as PLT


class TaylorDiagram(object):
    """
    Taylor diagram.

    Plot model standard deviation and correlation to reference (data)
    sample in a single-quadrant polar plot, with r=stddev and
    theta=arccos(correlation).
    """

    def __init__(self, refstd,
                 fig=None, rect=111, label='_', srange=(0, 1.5), extend=False):
        """
        Set up Taylor diagram axes, i.e. single quadrant polar
        plot, using `mpl_toolkits.axisartist.floating_axes`.

        Parameters:

        * refstd: reference standard deviation to be compared to
        * fig: input Figure or None
        * rect: subplot definition
        * label: reference label
        * srange: stddev axis extension, in units of *refstd*
        * extend: extend diagram to negative correlations
        """

        from matplotlib.projections import PolarAxes
        import mpl_toolkits.axisartist.floating_axes as FA
        import mpl_toolkits.axisartist.grid_finder as GF

        self.refstd = refstd            # Reference standard deviation

        tr = PolarAxes.PolarTransform()

        # Correlation labels
        rlocs = NP.array([0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1])
        if extend:
            # Diagram extended to negative correlations
            self.tmax = NP.pi
            rlocs = NP.concatenate((-rlocs[:0:-1], rlocs))
        else:
            # Diagram limited to positive correlations
            self.tmax = NP.pi/2
        tlocs = NP.arccos(rlocs)        # Conversion to polar angles
        gl1 = GF.FixedLocator(tlocs)    # Positions
        tf1 = GF.DictFormatter(dict(zip(tlocs, map(str, rlocs))))

        # Standard deviation axis extent (in units of reference stddev)
        self.smin = srange[0] * self.refstd
        self.smax = srange[1] * self.refstd

        ghelper = FA.GridHelperCurveLinear(
            tr,
            extremes=(0, self.tmax, self.smin, self.smax),
            grid_locator1=gl1, tick_formatter1=tf1)

        if fig is None:
            fig = PLT.figure()

        ax = FA.FloatingSubplot(fig, rect, grid_helper=ghelper)
        fig.add_subplot(ax)

        # Adjust axes
        ax.axis["top"].set_axis_direction("bottom")   # "Angle axis"
        ax.axis["top"].toggle(ticklabels=True, label=True)
        ax.axis["top"].major_ticklabels.set_axis_direction("top")
        ax.axis["top"].label.set_axis_direction("top")
        ax.axis["top"].label.set_text("Correlation")

        ax.axis["left"].set_axis_direction("bottom")  # "X axis"
        ax.axis["left"].label.set_text("Standard deviation")

        ax.axis["right"].set_axis_direction("top")    # "Y-axis"
        ax.axis["right"].toggle(ticklabels=True)
        ax.axis["right"].major_ticklabels.set_axis_direction(
            "bottom" if extend else "left")

        if self.smin:
            ax.axis["bottom"].toggle(ticklabels=False, label=False)
        else:
            ax.axis["bottom"].set_visible(False)          # Unused

        self._ax = ax                   # Graphical axes
        self.ax = ax.get_aux_axes(tr)   # Polar coordinates

        # Add reference point and stddev contour
        l, = self.ax.plot([0], self.refstd, 'k*',
                          ls='', ms=10, label=label)
        t = NP.linspace(0, self.tmax)
        r = NP.zeros_like(t) + self.refstd
        self.ax.plot(t, r, 'k--', label='_')

        # Collect sample points for latter use (e.g. legend)
        self.samplePoints = [l]

    def add_sample(self, stddev, corrcoef, *args, **kwargs):
        """
        Add sample (*stddev*, *corrcoeff*) to the Taylor
        diagram. *args* and *kwargs* are directly propagated to the
        `Figure.plot` command.
        """

        l, = self.ax.plot(NP.arccos(corrcoef), stddev,
                          *args, **kwargs)  # (theta, radius)
        self.samplePoints.append(l)

        return l

    def add_grid(self, *args, **kwargs):
        """Add a grid."""

        self._ax.grid(*args, **kwargs)

    def add_contours(self, levels=5, **kwargs):
        """
        Add constant centered RMS difference contours, defined by *levels*.
        """

        rs, ts = NP.meshgrid(NP.linspace(self.smin, self.smax),
                             NP.linspace(0, self.tmax))
        # Compute centered RMS difference
        rms = NP.sqrt(self.refstd**2 + rs**2 - 2*self.refstd*rs*NP.cos(ts))

        contours = self.ax.contour(ts, rs, rms, levels, **kwargs)

        return contours


def test2():
    """
    Climatology-oriented example (after iteration w/ Michael A. Rawlins).
    """

    # Reference std
    stdref = 254.89

    # Samples std,rho,name
    samples = [[211.19, 0.75, "LSTM"],
               [194.66, 0.74, "LSTM-KF"],
               [195.1,0.78, "LSTM-KFS"],
               [202.06, 0.72, "LSTM-UKF"],
               [197.44, 0.79, "LSTM-UKFS"]]
               
    fig = PLT.figure(figsize=(15, 10))

   # fig = PLT.figure()

    dia = TaylorDiagram(stdref, fig=fig, label='Reference',  rect=111) #extend=True)
    dia.samplePoints[0].set_color('r')  # Mark reference point as a red star

    # Add models to Taylor diagram
    for i, (stddev, corrcoef, name) in enumerate(samples):
        dia.add_sample(stddev, corrcoef,
                       marker='$%d$' % (i+1), ms=10, ls='',
                       mfc='k', mec='k',
                       label=name)

    # Add RMS contours, and label them
    contours = dia.add_contours(levels=5, colors='0.5')  # 5 levels in grey
    PLT.clabel(contours, inline=3, fontsize=7, fmt='%.0f')

    dia.add_grid()                                  # Add grid
    dia._ax.axis[:].major_ticks.set_tick_out(True)  # Put ticks outward

    # Add a figure legend and title
    fig.legend(dia.samplePoints,
               [ p.get_label() for p in dia.samplePoints ],
               numpoints=1, prop=dict(size='16'), loc='upper right')
    fig.suptitle("Taylor diagram", size='x-large')  # Figure title

    return dia


if __name__ == '__main__':

    dia = test2()

    PLT.show()

###########3lstm-knn
#https://github.com/kwokmoonho/Machine-Learning-SP500/blob/master/stock.py

######## https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/
## lstm-conv  lstm-cnn

###33lstm svm
##### https://github.com/nguyenthanhhoan/LSTM-SVMPrediction/blob/master/Raw_LST_SVM_withFilter_Prediction.ipynb

#### https://github.com/EsmeYi/time-series-forcasting
